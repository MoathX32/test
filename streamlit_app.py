import os
import io
import logging
import streamlit as st
from fastapi import HTTPException
from pydantic import BaseModel
from typing import List
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
import google.generativeai as genai

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()
genai_api_key = os.getenv("GENAI_API_KEY")

# Configure GenAI
genai.configure(api_key=genai_api_key)

# Initialize session state
if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}
if "reference_texts_store" not in st.session_state:
    st.session_state.reference_texts_store = {}
if "document_store" not in st.session_state:
    st.session_state.document_store = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # Store the chat history
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False  # Flag to check if processing is done
if "response_submitted" not in st.session_state:
    st.session_state.response_submitted = False  # Flag to check if a response has been submitted
if "sources_shown" not in st.session_state:
    st.session_state.sources_shown = False  # Flag to check if sources have been shown

# Function Definitions
def get_single_pdf_chunks(pdf_bytes, filename, text_splitter):
    if not pdf_bytes:
        raise HTTPException(status_code=400, detail="Empty PDF content.")
        
    pdf_stream = io.BytesIO(pdf_bytes)
    pdf_reader = PdfReader(pdf_stream)
    pdf_chunks = []
    for page_num, page in enumerate(pdf_reader.pages):
        page_text = page.extract_text()
        if page_text:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                document = Document(page_content=chunk, metadata={"page": page_num, "filename": filename})
                logging.info(f"Adding document chunk with metadata: {document.metadata}")
                pdf_chunks.append(document)
    return pdf_chunks

def get_all_pdfs_chunks(pdf_docs_with_names):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=990
    )

    all_chunks = []
    for pdf_bytes, filename in pdf_docs_with_names:
        pdf_chunks = get_single_pdf_chunks(pdf_bytes, filename, text_splitter)
        all_chunks.extend(pdf_chunks)
    return all_chunks

def read_files_from_folder(folder_path):
    pdf_docs_with_names = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            with open(os.path.join(folder_path, filename), "rb") as file:
                pdf_docs_with_names.append((file.read(), filename))
    return pdf_docs_with_names

def get_vector_store(documents):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    try:
        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
        return vectorstore
    except Exception as e:
        logging.warning("Issue with creating the vector store.")
        raise HTTPException(status_code=500, detail="Issue with creating the vector store.")

def process_lessons_and_video():
    folder_path = "./Data"  # Automatically set to the "Data" folder in the current directory
    playlist_url = "https://www.youtube.com/watch?v=DFyPl2cZM2g&list=PLX1bW_GeBRhDkTf_jbdvBbkHs2LCWVeXZ"  # Always set to the provided URL

    pdf_docs_with_names = read_files_from_folder(folder_path)
    if not pdf_docs_with_names or any(len(pdf) == 0 for pdf, _ in pdf_docs_with_names):
        raise HTTPException(status_code=400, detail="One or more PDF files are empty.")

    documents = get_all_pdfs_chunks(pdf_docs_with_names)
    pdf_vectorstore = get_vector_store(documents)

    playlist_id = playlist_url.split("list=")[-1].split("&")[0]

    st.session_state.vector_stores["pdf_vectorstore"] = pdf_vectorstore
    st.session_state.vector_stores["playlist_id"] = playlist_id
    st.session_state.document_store.extend(documents)  # Store original documents

    st.success("PDFs and playlist processed successfully")

class QueryRequest(BaseModel):
    query: str

def get_response(context, question, model):
    # Retrieve the existing chat history from session state
    chat_history = st.session_state.chat_history
    
    # Format the chat history in the correct structure for Google Generative AI
    formatted_history = [
        {"role": message["role"], "parts": [{"text": message["content"]}]}
        for message in chat_history
    ]
    
    # Start a new chat session, passing in the formatted chat history
    chat_session = model.start_chat(history=formatted_history)

    # Define the prompt template
    prompt_template = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰. ØªÙÙ‡Ù… Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©ØŒ ÙˆØ§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.
    Ù„Ø§ØªØ¬Ø¨ Ø§Ù„Ø§ Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­ Ø§Ùˆ Ø§Ø³ØªÙÙ‡Ù… Ù…Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ù‚ØµÙˆØ¯
    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ ÙÙ‡Ù…Ùƒ Ù„Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ . Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰.
    ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù…ÙÙ‡ÙˆÙ…Ù‹Ø§.
    Ù„Ø§ ØªØ¬Ø¨ Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ ÙÙ‡Ù…Ùƒ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Øµ.
    ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù† ØªÙƒÙˆÙ† Ø§Ù…Ø«Ù„Ù‡ ÙˆØªØ¹Ø±ÙŠÙØ§Øª Ù„Ø§ÙŠØ´ØªØ±Ø· Ø§Ù† ØªÙƒÙˆÙ† Ù…Ø°ÙƒÙˆØ±Ù‡ ÙÙŠ Ø§Ù„Ù†Øµ
    Ø§Ù„Ø³ÙŠØ§Ù‚: {context}\n
    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n
    """

    try:
        # Send the question and context to the model
        response = chat_session.send_message(prompt_template.format(context=context, question=question))
        response_text = response.text

        # Check for safety ratings
        if hasattr(response, 'safety_ratings') and response.safety_ratings:
            for rating in response.safety_ratings:
                if rating.probability != 'NEGLIGIBLE':
                    logging.warning("Response flagged due to safety concerns.")
                    return "", None, None

        # Log and display the response
        logging.info(f"AI Response: {response_text}")

        # Append the question and response to the chat history
        chat_history.append({"role": "user", "content": question})
        chat_history.append({"role": "model", "content": response_text})

        # Store the updated chat history in the session state
        st.session_state.chat_history = chat_history

        return response_text

    except Exception as e:
        logging.warning(e)
        return ""

# Display the chat history to the user
def display_chat_history():
    if "chat_history" in st.session_state and st.session_state.chat_history:
        st.write("### Conversation History")
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                st.write(f"**You**: {message['content']}")
            elif message["role"] == "model":
                st.write(f"**Assistant**: {message['content']}")
    else:
        st.write("No conversation history available.")

# Generate response UI
def generate_response(query_request: QueryRequest):
    if "pdf_vectorstore" not in st.session_state.vector_stores:
        st.error("PDFs must be processed first before generating a response.")
        return

    pdf_vectorstore = st.session_state.vector_stores['pdf_vectorstore']
    
    # Search for relevant content from the PDFs
    relevant_content = pdf_vectorstore.similarity_search(query_request.query, k=20)
    
    st.session_state.vector_stores["relevant_content"] = relevant_content

    # Combine the relevant content for context
    context = " ".join([doc.page_content for doc in relevant_content])

    # Set up the model generation config
    generation_config = {
        "temperature": 0.2,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8000,
    }

    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config=generation_config,
        system_instruction="You are a helpful document answering assistant."
    )
    
    # Get the response from the model based on the context and the user's query
    response = get_response(context, query_request.query, model)
    st.session_state.vector_stores["response_text"] = response  # Store the response for later use

    # Display the chat history
    display_chat_history()

    return response

# Streamlit UI Components
st.title("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙ Ø§Ù„Ø±Ø§Ø¨Ø¹")

if st.button('ğŸš€ Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ğŸš€'):
    with st.spinner('Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...'):
       process_lessons_and_video()  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
