import os
import io
import json
import logging
import streamlit as st
from fastapi import HTTPException
from pydantic import BaseModel
from typing import List, Dict
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langdetect import detect, LangDetectException
import google.generativeai as genai
import re
import difflib

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()
genai_api_key = os.getenv("GENAI_API_KEY")

# Configure GenAI
genai.configure(api_key=genai_api_key)

# Initialize session state variables
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False

if "response_submitted" not in st.session_state:
    st.session_state.response_submitted = False

if "sources_shown" not in st.session_state:
    st.session_state.sources_shown = False

if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}

if "reference_texts_store" not in st.session_state:
    st.session_state.reference_texts_store = {}

if "document_store" not in st.session_state:
    st.session_state.document_store = []

if "response_text" not in st.session_state:
    st.session_state.response_text = ""

# Function Definitions

def get_single_pdf_chunks(pdf_bytes, filename, text_splitter):
    if not pdf_bytes:
        raise HTTPException(status_code=400, detail="Empty PDF content.")
        
    pdf_stream = io.BytesIO(pdf_bytes)
    pdf_reader = PdfReader(pdf_stream)
    pdf_chunks = []
    for page_num, page in enumerate(pdf_reader.pages):
        page_text = page.extract_text()
        if page_text:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                document = Document(page_content=chunk, metadata={"page": page_num, "filename": filename})
                logging.info(f"Adding document chunk with metadata: {document.metadata}")
                pdf_chunks.append(document)
    return pdf_chunks

def get_all_pdfs_chunks(pdf_docs_with_names):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=990
    )

    all_chunks = []
    for pdf_bytes, filename in pdf_docs_with_names:
        pdf_chunks = get_single_pdf_chunks(pdf_bytes, filename, text_splitter)
        all_chunks.extend(pdf_chunks)
    return all_chunks

def read_files_from_folder(folder_path):
    pdf_docs_with_names = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            with open(os.path.join(folder_path, filename), "rb") as file:
                pdf_docs_with_names.append((file.read(), filename))
    return pdf_docs_with_names

def get_vector_store(documents):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    try:
        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
        return vectorstore
    except Exception as e:
        logging.warning("Issue with creating the vector store.")
        raise HTTPException(status_code=500, detail="Issue with creating the vector store.")

def process_lessons():
    folder_path = "./Data"  # Automatically set to the "Data" folder in the current directory

    pdf_docs_with_names = read_files_from_folder(folder_path)
    if not pdf_docs_with_names or any(len(pdf) == 0 for pdf, _ in pdf_docs_with_names):
        raise HTTPException(status_code=400, detail="One or more PDF files are empty.")

    documents = get_all_pdfs_chunks(pdf_docs_with_names)
    pdf_vectorstore = get_vector_store(documents)

    st.session_state.vector_stores["pdf_vectorstore"] = pdf_vectorstore
    st.session_state.document_store.extend(documents)  # Store original documents

    st.success("ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ø¨Ù†Ø¬Ø§Ø­")

class QueryRequest(BaseModel):
    query: str

def get_response(context, question, model):
    chat_session = model.start_chat(history=[])

    # Updated prompt for response generation
    prompt_template = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø£Ø¬Ø¨ Ø¹Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ Ø§Ù„Ù…ØªØ§Ø­.
    ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø¥Ø¶Ø§ÙÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø£Ù…Ø«Ù„Ø© ØªØ¯Ø¹Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø¨Ø´Ø±Ø· Ø£Ù† ØªØ¨Ù‚Ù‰ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÙˆÙ† Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù†Ù‡.

    Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ: {context}\n
    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n
    """

    try:
        response = chat_session.send_message(prompt_template.format(context=context, question=question))
        response_text = response.text

        if hasattr(response, 'safety_ratings') and response.safety_ratings:
            for rating in response.safety_ratings:
                if rating.probability != 'NEGLIGIBLE':
                    logging.warning("ØªÙ… Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„Ø£Ø³Ø¨Ø§Ø¨ ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø³Ù„Ø§Ù…Ø©.")
                    return "", None, None

        logging.info(f"Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {response_text}")
        return response_text
    except Exception as e:
        logging.warning(e)
        return ""


def generate_response(query_request: QueryRequest):
    if "pdf_vectorstore" not in st.session_state.vector_stores:
        st.error("ÙŠØ¬Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.")
        return

    pdf_vectorstore = st.session_state.vector_stores['pdf_vectorstore']
    
    relevant_content = pdf_vectorstore.similarity_search(query_request.query, k=20)
    
    st.session_state.vector_stores["relevant_content"] = relevant_content

    context = " ".join([doc.page_content for doc in relevant_content])

    generation_config = {
        "temperature": 0.2,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8000,
    }

    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=generation_config,
        system_instruction="Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
    )
    
    response = get_response(context, query_request.query, model)
    st.session_state.response_text = response  # Store the response text for generating questions
    return response

def generate_questions_from_response(num_questions, question_type, model):
    # Use the AI-generated response text to generate questions
    response_text = st.session_state.response_text
    
    if not response_text:
        st.error("Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.")
        return None

    if question_type == "MCQ":
        # Updated prompt for intelligent question generation
        prompt_template = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ {num_questions} Ù…Ù† Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ (MCQs) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©.
        ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø£Ù…Ø«Ù„Ø© Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…. 
        ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø¹Ù„Ù‰ 4 Ø®ÙŠØ§Ø±Ø§Øª ÙˆØ¥Ø¬Ø§Ø¨Ø© ØµØ­ÙŠØ­Ø©ØŒ ÙˆÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ù‚Ù‰ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÙˆÙ† Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù†Ù‡.

        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response_text}\n
        """
    else:
        # Updated prompt for intelligent True/False question generation
        prompt_template = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ {num_questions} Ù…Ù† Ø£Ø³Ø¦Ù„Ø© ØµØ­/Ø®Ø·Ø£ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©.
        ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆØªØ³Ø§Ù‡Ù… ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‡Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ØŒ ÙˆÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ø£Ù…Ø«Ù„Ø© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±.
        ÙŠØ¬Ø¨ Ø£Ù† ØªØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª ØµØ­ÙŠØ­Ø©ØŒ ÙˆÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ù‚Ù‰ Ø¶Ù…Ù† Ø¥Ø·Ø§Ø± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÙˆÙ† Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù†Ù‡.

        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response_text}\n
        """

    try:
        response = model.start_chat(history=[]).send_message(prompt_template)
        response_text = response.text.strip()

        # Log the response text to debug potential issues
        logging.info(f"AI Model Response for questions: {response_text}")

        if response_text:
            response_json = clean_json_response(response_text)
            if response_json:
                return response_json
            else:
                logging.error("Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù… ØªÙƒÙ† Ø¨ØµÙŠØºØ© JSON. ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„ØµÙŠØºØ© ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
                st.error("Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ØµÙŠØºØ© JSON. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
                return None
        else:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø£ÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
            return None
    except Exception as e:
        logging.warning(f"Ø®Ø·Ø£: {e}")
        return None

def clean_json_response(response_text):
    try:
        response_json = json.loads(response_text)
        return response_json
    except json.JSONDecodeError:
        try:
            cleaned_text = re.sub(r'```json', '', response_text).strip()
            cleaned_text = re.sub(r'```', '', cleaned_text).strip()

            match = re.search(r'(\{.*\}|\[.*\])', cleaned_text, re.DOTALL)
            if match:
                cleaned_text = match.group(0)
                response_json = json.loads(cleaned_text)
                return response_json
            else:
                logging.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ø¦Ù† JSON ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©")
                return None
        except (ValueError, json.JSONDecodeError) as e:
            logging.error(f"Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„ÙŠØ³Øª JSON ØµØ§Ù„Ø­Ø©: {str(e)}")
            return None

# Streamlit UI Components

st.title("Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF")

# Section to process PDFs
if st.button('ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF ğŸš€'):
    with st.spinner('Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...'):
       process_lessons()  # Call the function to process PDFs
    st.session_state.processing_complete = True  # Update session state

# Section for asking a query
if st.session_state.processing_complete:
    with st.form(key='response_form'):
        query = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:")
        response_button = st.form_submit_button(label='Ø¥Ø±Ø³Ø§Ù„')

        if response_button:
            query_request = QueryRequest(query=query)
            response = generate_response(query_request)
            st.write("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", response)

# Section to generate questions
if st.session_state.processing_complete:
    if st.session_state.response_text:
        with st.form(key='questions_form'):
            question_type = st.selectbox("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„:", ["MCQ", "ØµØ­/Ø®Ø·Ø£"])
            questions_number = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:", min_value=1, max_value=10)
            generate_button = st.form_submit_button(label='ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©')

            if generate_button:
                model = genai.GenerativeModel(
                    model_name="gemini-1.5-flash",
                    generation_config={"temperature": 0.2, "top_p": 1, "top_k": 1, "max_output_tokens": 8000}
                )
                questions = generate_questions_from_response(questions_number, question_type, model)
                st.write("Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©:", questions)
