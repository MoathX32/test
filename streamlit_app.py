import os
import io
import json
import logging
import streamlit as st
from fastapi import HTTPException
from pydantic import BaseModel
from typing import List
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
import google.generativeai as genai
import re

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()
genai_api_key = os.getenv("GENAI_API_KEY")

# Configure GenAI
genai.configure(api_key=genai_api_key)

# Initialize session state variables
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False

if "response_submitted" not in st.session_state:
    st.session_state.response_submitted = False

if "sources_shown" not in st.session_state:
    st.session_state.sources_shown = False

if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}

if "reference_texts_store" not in st.session_state:
    st.session_state.reference_texts_store = {}

if "document_store" not in st.session_state:
    st.session_state.document_store = []

# Function Definitions
def get_single_pdf_chunks(pdf_bytes, filename, text_splitter):
    if not pdf_bytes:
        raise HTTPException(status_code=400, detail="Empty PDF content.")
        
    pdf_stream = io.BytesIO(pdf_bytes)
    pdf_reader = PdfReader(pdf_stream)
    pdf_chunks = []
    for page_num, page in enumerate(pdf_reader.pages):
        page_text = page.extract_text()
        if page_text:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                document = Document(page_content=chunk, metadata={"page": page_num, "filename": filename})
                logging.info(f"Adding document chunk with metadata: {document.metadata}")
                pdf_chunks.append(document)
    return pdf_chunks

def get_all_pdfs_chunks(pdf_docs_with_names):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=990
    )

    all_chunks = []
    for pdf_bytes, filename in pdf_docs_with_names:
        pdf_chunks = get_single_pdf_chunks(pdf_bytes, filename, text_splitter)
        all_chunks.extend(pdf_chunks)
    return all_chunks

def read_files_from_folder(folder_path):
    pdf_docs_with_names = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            with open(os.path.join(folder_path, filename), "rb") as file:
                pdf_docs_with_names.append((file.read(), filename))
    return pdf_docs_with_names

def get_vector_store(documents):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    try:
        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
        return vectorstore
    except Exception as e:
        logging.warning("Issue with creating the vector store.")
        raise HTTPException(status_code=500, detail="Issue with creating the vector store.")

def process_lessons_and_video():
    folder_path = "./Data"  # Automatically set to the "Data" folder in the current directory

    pdf_docs_with_names = read_files_from_folder(folder_path)
    if not pdf_docs_with_names or any(len(pdf) == 0 for pdf, _ in pdf_docs_with_names):
        raise HTTPException(status_code=400, detail="One or more PDF files are empty.")

    documents = get_all_pdfs_chunks(pdf_docs_with_names)
    pdf_vectorstore = get_vector_store(documents)

    st.session_state.vector_stores["pdf_vectorstore"] = pdf_vectorstore
    st.session_state.document_store.extend(documents)  # Store original documents

    st.success("PDFs processed successfully")

class QueryRequest(BaseModel):
    query: str

def get_response(context, question, model):
    chat_session = model.start_chat(history=[])

    prompt_template = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰. ØªÙÙ‡Ù… Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©ØŒ ÙˆØ§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.
    Ù„Ø§ØªØ¬Ø¨ Ø§Ù„Ø§ Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­ Ø§Ùˆ Ø§Ø³ØªÙÙ‡Ù… Ù…Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ù‚ØµÙˆØ¯
    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ ÙÙ‡Ù…Ùƒ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ . Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰.
    ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù…ÙÙ‡ÙˆÙ…Ù‹Ø§.
    Ù„Ø§ ØªØ¬Ø¨ Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ ÙÙ‡Ù…Ùƒ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Øµ.
    Ø§Ù„Ø³ÙŠØ§Ù‚: {context}\n
    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n
    """


    try:
        response = chat_session.send_message(prompt_template.format(context=context, question=question))
        response_text = response.text

        if hasattr(response, 'safety_ratings') and response.safety_ratings:
            for rating in response.safety_ratings:
                if rating.probability != 'NEGLIGIBLE':
                    logging.warning("Response flagged due to safety concerns.")
                    return "", None, None

        logging.info(f"AI Response: {response_text}")
        return response_text
    except Exception as e:
        logging.warning(e)
        return ""

def generate_response(query_request: QueryRequest):
    if "pdf_vectorstore" not in st.session_state.vector_stores:
        st.error("PDFs must be processed first before generating a response.")
        return

    pdf_vectorstore = st.session_state.vector_stores['pdf_vectorstore']
    
    relevant_content = pdf_vectorstore.similarity_search(query_request.query, k=20)
    
    st.session_state.vector_stores["relevant_content"] = relevant_content

    context = " ".join([doc.page_content for doc in relevant_content])

    generation_config = {
        "temperature": 0.2,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8000,
    }

    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config=generation_config,
        system_instruction="You are a helpful document answering assistant."
    )
    
    response = get_response(context, query_request.query, model)
    st.session_state.vector_stores["response_text"] = response  # Store the response for later use
    return response

def clean_json_response(response_text):
    try:
        response_json = json.loads(response_text)
        return response_json
    except json.JSONDecodeError:
        try:
            cleaned_text = re.sub(r'```json', '', response_text).strip()
            cleaned_text = re.sub(r'```', '', cleaned_text).strip()

            match = re.search(r'(\{.*\}|\[.*\])', cleaned_text, re.DOTALL)
            if match:
                cleaned_text = match.group(0)
                response_json = json.loads(cleaned_text)
                return response_json
            else:
                logging.error("No JSON object or array found in response")
                return None
        except (ValueError, json.JSONDecodeError) as e:
            logging.error(f"Response is not a valid JSON: {str(e)}")
            return None

def extract_reference_texts_as_json(response_text, context):
    ref_prompt = f"""
    Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø­Ø¯Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    Ù‚Ø¯Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¯Ø±Ø³ ÙƒÙ…ÙØªØ§Ø­ 'filename'ØŒ ÙˆØ£Ø¶Ù Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙÙ‚Ø· ØªØ­Øª Ù…ÙØªØ§Ø­ 'relevant_texts'.
    
    Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response_text}

    Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
    {context}

    Ù‚Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ù…Ø¹ Ø¨ÙŠØ§Ù† Ù…Ø±Ø¬Ø¹Ù‡ ÙÙŠ Ø´ÙƒÙ„ JSON ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¹Ù„Ø§Ù‡.
    """

    chat_session = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config={
            "temperature": 0.2,
            "top_p": 1,
            "top_k": 1,
            "max_output_tokens": 8000,
        }
    ).start_chat(history=[])
    
    ref_response = chat_session.send_message(ref_prompt)
    ref_response_text = ref_response.text.strip()

    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ„Ù… Ù„ÙØ­ØµÙ‡
    logging.info(f"Reference response text: {ref_response_text}")

    reference_texts_json = clean_json_response(ref_response_text)
    
    if reference_texts_json is None:
        logging.warning("Failed to parse JSON from reference response.")
    else:
        logging.info(f"Parsed reference texts JSON: {reference_texts_json}")
    
    return reference_texts_json

def generate_reference_texts():
    if "pdf_vectorstore" not in st.session_state.vector_stores or "response_text" not in st.session_state.vector_stores or "relevant_content" not in st.session_state.vector_stores:
        raise HTTPException(status_code=400, detail="PDFs, response, and relevant content must be processed first.")
    
    response_text = st.session_state.vector_stores['response_text']

    context = " ".join([doc.page_content for doc in st.session_state.vector_stores["relevant_content"]])

    reference_texts = extract_reference_texts_as_json(response_text, context)
    
    if reference_texts is None:
        logging.warning("No relevant reference texts found.")
        st.session_state.reference_texts_store["last_reference_texts"] = None
    else:
        st.session_state.reference_texts_store["last_reference_texts"] = {"reference_texts": reference_texts}
    
    return reference_texts

class QuestionRequest(BaseModel):
    question_type: str
    questions_number: int


def generate_questions(relevant_text, num_questions, question_type, model):
    if not relevant_text.strip():
        logging.warning("Relevant text is empty or invalid.")
        st.error("Relevant text is empty or invalid.")
        return None

    if question_type == "MCQ":
        prompt_template = f"""
        You are an AI assistant tasked with generating exactly {num_questions} multiple-choice questions (MCQs) from the given context. 
        Create a set of MCQs with 4 answer options each. Ensure that the questions cover key concepts from the context provided.
        The questions should be formatted in JSON with fields 'question', 'options', and 'correct_answer'.
        
        Context: {relevant_text}\n
        """
    else:
        prompt_template = f"""
        You are an AI assistant tasked with generating exactly {num_questions} true/false questions from the given context. 
        The questions should be formatted in JSON with fields 'question' and 'correct_answer'. 
        
        Context: {relevant_text}\n
        """

    try:
        response = model.start_chat(history=[]).send_message(prompt_template)
        response_text = response.text.strip()

        logging.info(f"Model Response: {response_text}")

        if response_text:
            response_json = clean_json_response(response_text)
            return response_json if response_json else None
        else:
            logging.warning("Received an empty response from the model.")
            st.error("Received an empty response from the model.")
            return None
    except Exception as e:
        logging.warning(f"Error: {e}")
        st.error(f"Error generating questions: {e}")
        return None
    
def generate_questions_endpoint(question_request: QuestionRequest):
    if "last_reference_texts" not in st.session_state.reference_texts_store:
        raise HTTPException(status_code=400, detail="No reference texts found. Please process the reference texts first.")
    
    reference_texts = st.session_state.reference_texts_store.get("last_reference_texts", {})
    
    # Ensure that the reference texts are properly extracted
    if "reference_texts" in reference_texts and isinstance(reference_texts["reference_texts"], dict):
        relevant_texts = reference_texts["reference_texts"].get("relevant_texts", "")
        
        if not relevant_texts.strip():
            logging.error("Relevant texts are empty or invalid.")
            st.error("Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© ØºÙŠØ± ØµØ­ÙŠØ­Ø©.")
            return None

        logging.info(f"Relevant texts extracted: {relevant_texts}")
    else:
        st.error("Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© ØºÙŠØ± ØµØ­ÙŠØ­Ø©.")
        logging.error(f"Reference texts structure: {reference_texts}")
        return None

    questions_json = generate_questions(
        relevant_text=relevant_texts,
        num_questions=question_request.questions_number,
        question_type=question_request.question_type,
        model=genai.GenerativeModel(
            model_name="gemini-1.5-pro-latest",
            generation_config={
                "temperature": 0.2,
                "top_p": 1,
                "top_k": 1,
                "max_output_tokens": 8000,
            },
            system_instruction="You are a helpful document answering assistant."
        )
    )

    return questions_json

# Streamlit UI Components
import streamlit as st

# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
st.title("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙ Ø§Ù„Ø±Ø§Ø¨Ø¹")

# Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
with st.expander("Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"):
    st.write("""
    **ØªÙ†Ø¨ÙŠÙ‡:**
    Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ø§Ø²Ø§Ù„ ØªØ­Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ ÙˆÙ‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ù…ÙŠØ²Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©. Ù†Ù‚Ø¯Ø± ØªÙÙ‡Ù…Ùƒ ÙˆØ£ÙŠ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‚Ø¯ ØªØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.

    **Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ÙˆØ§Ø¬Ù‡Ø© Streamlit:**
    1. **ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:**
       Ø¹Ù†Ø¯ ÙØªØ­ ÙˆØ§Ø¬Ù‡Ø© StreamlitØŒ Ø³ØªØ¬Ø¯ Ø²Ø±Ù‹Ø§ Ø¨Ø¹Ù†ÙˆØ§Ù† "Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯". Ø¨Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø²Ø±ØŒ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Data.
    2. **Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„:**
       ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø­Ù‚Ù„ Ø§Ù„Ù†Øµ "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ". Ø¨Ø¹Ø¯ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± "Ø£Ø¬Ø¨". Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ù…Ù„ÙØ§Øª PDF ÙˆÙŠØ¹Ø±Ø¶ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„.
    3. **Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±:**
       ÙÙŠ Ù‚Ø³Ù… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡ (Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© "MCQ" Ø£Ùˆ ØµØ­/Ø®Ø·Ø£ "True/False").
       Ø­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¤Ø´Ø±ØŒ Ø«Ù… Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø§Ø¨Ø¯Ø£ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±". Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©.
    """)

st.write("---")

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø­Ø§Øª ÙØ§Ø±ØºØ© Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ù„ØªÙˆØ³ÙŠØ· Ø§Ù„Ø²Ø± Ø¹Ù…ÙˆØ¯ÙŠÙ‹Ø§
st.write("")
st.write("")
st.write("")


# Ø§Ø³ØªØ®Ø¯Ø§Ù… st.button Ù…Ø¹ Ù†ÙØ³ Ø§Ù„Ù†Øµ Ù„ØªÙ‚Ø¯ÙŠÙ… Ù†ÙØ³ Ø§Ù„ÙˆØ¸ÙŠÙØ©
if st.button('ğŸš€ Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ğŸš€'):
    with st.spinner('Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...'):
       process_lessons_and_video()  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    st.session_state.processing_complete = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

st.write("---")

# Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ response_form ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
if st.session_state.processing_complete:
    with st.form(key='response_form'):
        query = st.text_input("ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ:")
        response_button = st.form_submit_button(label='Ø£Ø¬Ø¨')

        if response_button:
            query_request = QueryRequest(query=query)
            response = generate_response(query_request)
            st.write("Ø§Ù„Ø±Ø¯:", response)
            st.session_state.response_submitted = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø±Ø¯

    st.write("---")

    # Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø±Ø¯ØŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    if st.session_state.response_submitted:
        reference_texts = generate_reference_texts()
        if reference_texts is not None:
            st.session_state.sources_shown = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø±

    st.write("---")

    # Ø¥Ø¸Ù‡Ø§Ø± Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø±Ø¯
    if st.session_state.sources_shown:
        # Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        with st.form(key='questions_form'):
            question_type = st.selectbox("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„:", ["MCQ", "True/False"])
            questions_number = st.number_input("Ø§Ø®ØªØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:", min_value=1, max_value=30)
            generate_questions_button = st.form_submit_button(label='Ø§Ø¨Ø¯Ø£ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±')

            if generate_questions_button:
                question_request = QuestionRequest(question_type=question_type, questions_number=questions_number)
                questions = generate_questions_endpoint(question_request)
                st.write("Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", questions)
