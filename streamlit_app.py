import os
import io
import json
import logging
import streamlit as st
from fastapi import HTTPException
from pydantic import BaseModel
from typing import List, Dict
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
import google.generativeai as genai
import re

# Configure logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()
genai_api_key = os.getenv("GENAI_API_KEY")

# Configure GenAI
genai.configure(api_key=genai_api_key)

# Initialize global stores
if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}
if "reference_texts_store" not in st.session_state:
    st.session_state.reference_texts_store = {}
if "document_store" not in st.session_state:
    st.session_state.document_store = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # Initialize chat history

# Function Definitions
def get_single_pdf_chunks(pdf_bytes, filename, text_splitter):
    if not pdf_bytes:
        raise HTTPException(status_code=400, detail="Empty PDF content.")
        
    pdf_stream = io.BytesIO(pdf_bytes)
    pdf_reader = PdfReader(pdf_stream)
    pdf_chunks = []
    for page_num, page in enumerate(pdf_reader.pages):
        page_text = page.extract_text()
        if page_text:
            page_chunks = text_splitter.split_text(page_text)
            for chunk in page_chunks:
                document = Document(page_content=chunk, metadata={"page": page_num, "filename": filename})
                logging.info(f"Adding document chunk with metadata: {document.metadata}")
                pdf_chunks.append(document)
    return pdf_chunks

def get_all_pdfs_chunks(pdf_docs_with_names):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=990
    )

    all_chunks = []
    for pdf_bytes, filename in pdf_docs_with_names:
        pdf_chunks = get_single_pdf_chunks(pdf_bytes, filename, text_splitter)
        all_chunks.extend(pdf_chunks)
    return all_chunks

def read_files_from_folder(folder_path):
    pdf_docs_with_names = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            with open(os.path.join(folder_path, filename), "rb") as file:
                pdf_docs_with_names.append((file.read(), filename))
    return pdf_docs_with_names

def get_vector_store(documents):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    try:
        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
        return vectorstore
    except Exception as e:
        logging.warning("Issue with creating the vector store.")
        raise HTTPException(status_code=500, detail="Issue with creating the vector store.")

def process_lessons_and_video():
    folder_path = "./Data"  # Automatically set to the "Data" folder in the current directory
    playlist_url = "https://www.youtube.com/watch?v=DFyPl2cZM2g&list=PLX1bW_GeBRhDkTf_jbdvBbkHs2LCWVeXZ"  # Always set to the provided URL

    pdf_docs_with_names = read_files_from_folder(folder_path)
    if not pdf_docs_with_names or any(len(pdf) == 0 for pdf, _ in pdf_docs_with_names):
        raise HTTPException(status_code=400, detail="One or more PDF files are empty.")

    documents = get_all_pdfs_chunks(pdf_docs_with_names)
    pdf_vectorstore = get_vector_store(documents)

    playlist_id = playlist_url.split("list=")[-1].split("&")[0]

    st.session_state.vector_stores["pdf_vectorstore"] = pdf_vectorstore
    st.session_state.vector_stores["playlist_id"] = playlist_id
    st.session_state.document_store.extend(documents)  # Store original documents

    st.success("PDFs and playlist processed successfully")

class QueryRequest(BaseModel):
    query: str

def get_response(context, question, model):
    # Prepare the prompt by including the previous conversation history
    prompt_template = """
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰. ØªÙÙ‡Ù… Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©ØŒ ÙˆØ§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.
    Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ Ø£Ø¯Ù†Ø§Ù‡ ÙÙ‚Ø·. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰.
    ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù…Ø®ØªØµØ±Ù‹Ø§ ÙˆÙ…ÙÙ‡ÙˆÙ…Ù‹Ø§.
    Ù„Ø§ ØªØ¬Ø¨ Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø®Ø§Ø±Ø¬ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Øµ.
    
    Ø§Ù„Ø³ÙŠØ§Ù‚: {context}\n
    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n
    """

    # Initialize or update the chat history
    chat_history = st.session_state.get("chat_history", [])
    chat_history.append({"role": "user", "content": question})

    chat_session = model.start_chat(history=chat_history)

    try:
        # Send the message and get the response
        response = chat_session.send_message(prompt_template.format(context=context, question=question))
        response_text = response.text

        # Update chat history with the bot's response
        chat_history.append({"role": "assistant", "content": response_text})
        st.session_state.chat_history = chat_history

        logging.info(f"AI Response: {response_text}")
        return response_text
    except Exception as e:
        logging.warning(e)
        return ""

def generate_response(query_request: QueryRequest):
    if "pdf_vectorstore" not in st.session_state.vector_stores:
        st.error("PDFs must be processed first before generating a response.")
        return

    pdf_vectorstore = st.session_state.vector_stores['pdf_vectorstore']
    
    relevant_content = pdf_vectorstore.similarity_search(query_request.query, k=20)
    
    st.session_state.vector_stores["relevant_content"] = relevant_content

    context = " ".join([doc.page_content for doc in relevant_content])

    generation_config = {
        "temperature": 0.2,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8000,
    }

    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config=generation_config,
        system_instruction="You are a helpful document answering assistant."
    )
    
    response = get_response(context, query_request.query, model)
    st.session_state.vector_stores["response_text"] = response  # Store the response for later use
    return response

def extract_reference_texts_as_json(response_text, context):
    ref_prompt = f"""
    Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø­Ø¯Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¶Ù…Ù† Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¯Ø±ÙˆØ³ Ù…Ø«Ù„ "Ø¯Ø±Ø³: Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" Ø£Ùˆ "Ø¯Ø±Ø³: Ø£Ø¯Ø¨ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø¹Ø¨Ø§Ø³ÙŠ".
    Ù‚Ø¯Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¯Ø±Ø³ ÙƒÙ…ÙØªØ§Ø­ 'filename'ØŒ ÙˆØ£Ø¶Ù Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙÙ‚Ø· ØªØ­Øª Ù…ÙØªØ§Ø­ 'relevant_texts'.
    
    ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙŠØªØ¶Ù…Ù† 'filename' Ùˆ 'relevant_texts' ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­:
    [
        {{
            "filename": "Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¯Ø±Ø³",
            "relevant_texts": "Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ ÙÙ‚Ø·"
        }}
    ]

    Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {response_text}

    Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
    {context}

    Ù‚Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø±ØªØ¨Ø§Ø·Ù‹Ø§ Ù…Ø¹ Ø¨ÙŠØ§Ù† Ù…Ø±Ø¬Ø¹Ù‡ ÙÙŠ Ø´ÙƒÙ„ JSON ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¹Ù„Ø§Ù‡.
    """

    chat_session = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config={
            "temperature": 0.2,
            "top_p": 1,
            "top_k": 1,
            "max_output_tokens": 8000,
        }
    ).start_chat(history=[])
    
    ref_response = chat_session.send_message(ref_prompt)
    ref_response_text = ref_response.text.strip()

    try:
        reference_texts_json = json.loads(ref_response_text)
    except json.JSONDecodeError:
        reference_texts_json = None
    
    return reference_texts_json

def generate_reference_texts():
    if "pdf_vectorstore" not in st.session_state.vector_stores or "response_text" not in st.session_state.vector_stores or "relevant_content" not in st.session_state.vector_stores:
        raise HTTPException(status_code=400, detail="PDFs, response, and relevant content must be processed first.")
    
    response_text = st.session_state.vector_stores['response_text']

    context = " ".join([doc.page_content for doc in st.session_state.vector_stores["relevant_content"]])

    reference_texts = extract_reference_texts_as_json(response_text, context)
    
    if reference_texts is None:
        logging.warning("No relevant reference texts found.")
        st.session_state.reference_texts_store["last_reference_texts"] = None
    else:
        st.session_state.reference_texts_store["last_reference_texts"] = {"reference_texts": reference_texts}
    
    return reference_texts

def find_video_segment(filenames, response_text, playlist_id):
    videos = get_playlist_videos(playlist_id)
    relevant_video_urls = {}

    for filename in filenames:
        for video in videos:
            if filename.lower() in video['title'].lower():
                video_id = video['video_id']
                relevant_video_urls[filename] = f"https://www.youtube.com/watch?v={video_id}"
                break

    if not relevant_video_urls:
        logging.warning(f"No matching video found for lessons: {filenames}")
        return None

    return relevant_video_urls

def generate_video_segment_url():
    if "playlist_id" not in st.session_state.vector_stores or "last_reference_texts" not in st.session_state.reference_texts_store:
        logging.error("Required data not found: playlist_id or last_reference_texts.")
        raise HTTPException(status_code=400, detail="Playlist and reference texts must be processed first.")
    
    if st.session_state.reference_texts_store.get("last_reference_texts") is None:
        logging.error("Cannot generate video segment URLs because reference texts are None.")
        raise HTTPException(status_code=400, detail="Reference texts are None. Cannot generate video segment URLs.")

    playlist_id = st.session_state.vector_stores.get("playlist_id")
    reference_texts = st.session_state.reference_texts_store.get("last_reference_texts")
    
    filenames = [ref["filename"] for ref in reference_texts["reference_texts"]]
    response_text = st.session_state.vector_stores["response_text"]

    if not filenames:
        logging.error("Lesson names are missing from the reference texts.")
        return None

    video_segment_urls = find_video_segment(filenames, response_text, playlist_id)
    
    if not video_segment_urls:
        logging.error(f"Video segments not found for lessons: {filenames}.")
        raise HTTPException(status_code=404, detail="Not Found")

    return video_segment_urls

class QuestionRequest(BaseModel):
    question_type: str
    questions_number: int

def generate_questions(relevant_text, num_questions, question_type, model):
    if not relevant_text.strip():
        logging.warning("Relevant text is empty or invalid.")
        st.error("Relevant text is empty or invalid.")
        return None

    if question_type == "MCQ":
        prompt_template = f"""
        You are an AI assistant tasked with generating {num_questions} multiple-choice questions (MCQs) from the given context. \
        Create a set of MCQs with 4 answer options each. Ensure that the questions cover key concepts from the context provided and provide the correct answer as well. \
        Ensure the output is in JSON format with fields 'question', 'options', and 'correct_answer', and ensure the output language as context language.
        
        Context: {relevant_text}\n
        """
    else:
        prompt_template = f"""
        You are an AI assistant tasked with generating {num_questions} true/false questions from the given context. \
        For each true/false question, provide the correct answer as well. \
        Ensure the output is in JSON format with fields 'question' and 'correct_answer', and ensure the output language as context language.
        
        Context: {relevant_text}\n
        """

    try:
        response = model.start_chat(history=[]).send_message(prompt_template)
        response_text = response.text.strip()

        logging.info(f"Model Response: {response_text}")  # Log the model's response

        try:
            response_json = json.loads(response_text)
        except json.JSONDecodeError:
            response_json = None

        return response_json
    except Exception as e:
        logging.warning(f"Error: {e}")
        st.error(f"Error generating questions: {e}")
        return None

def generate_questions_endpoint(question_request: QuestionRequest):
    if "last_reference_texts" not in st.session_state.reference_texts_store:
        raise HTTPException(status_code=400, detail="No reference texts found. Please process the reference texts first.")
    
    if st.session_state.reference_texts_store.get("last_reference_texts") is None:
        logging.error("Cannot generate questions because reference texts are None.")
        raise HTTPException(status_code=400, detail="Reference texts are None. Cannot generate questions.")

    generation_config = {
        "temperature": 0.2,
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8000,
    }

    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        generation_config=generation_config,
        system_instruction="You are a helpful document answering assistant."
    )

    relevant_texts = " ".join([ref["relevant_texts"] for ref in st.session_state.reference_texts_store["last_reference_texts"]["reference_texts"]])

    questions_json = generate_questions(
        relevant_text=relevant_texts,
        num_questions=question_request.questions_number,
        question_type=question_request.question_type,
        model=model
    )

    return questions_json

def get_playlist_videos(playlist_id):
    # Mock implementation
    # This should be replaced with actual code that interacts with YouTube API to fetch playlist videos
    return [
        {"title": "Ø­Ù‚ÙˆÙ‚ÙŠ ÙˆÙˆØ§Ø¬Ø¨Ø§ØªÙŠ ÙÙŠ Ø§Ù„Ø¨ÙŠØª", "video_id": "abc123"},
        {"title": "ÙƒÙŠÙ ØªÙ†ØªØ®Ø¨ Ù…Ø¬Ù„Ø³ Ø§Ù„Ù‚Ø³Ù…ØŸ", "video_id": "def456"},
        {"title": "ÙƒÙŠÙ Ù†Ù…Ø§Ø±Ø³ Ù…ÙˆØ§Ø·Ù†ØªÙ†Ø§ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©ØŸ", "video_id": "ghi789"}
    ]

# Streamlit UI Components
import streamlit as st

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø§Øª ÙÙŠ session_state
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False

if "response_submitted" not in st.session_state:
    st.session_state.response_submitted = False

if "sources_shown" not in st.session_state:
    st.session_state.sources_shown = False

# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
st.title("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø§Ø¯Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØµÙ Ø§Ù„Ø±Ø§Ø¨Ø¹")

# Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
with st.expander("Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"):
    st.write("""
    **ØªÙ†Ø¨ÙŠÙ‡:**
    Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ø§Ø²Ø§Ù„ ØªØ­Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ ÙˆÙ‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ù…ÙŠØ²Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©. Ù†Ù‚Ø¯Ø± ØªÙÙ‡Ù…Ùƒ ÙˆØ£ÙŠ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‚Ø¯ ØªØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.

    **Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ÙˆØ§Ø¬Ù‡Ø© Streamlit:**
    1. **ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:**
       Ø¹Ù†Ø¯ ÙØªØ­ ÙˆØ§Ø¬Ù‡Ø© StreamlitØŒ Ø³ØªØ¬Ø¯ Ø²Ø±Ù‹Ø§ Ø¨Ø¹Ù†ÙˆØ§Ù† "Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯". Ø¨Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø²Ø±ØŒ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Data ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© ØªØ´ØºÙŠÙ„ YouTube Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.
    2. **Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„:**
       ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø­Ù‚Ù„ Ø§Ù„Ù†Øµ "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ". Ø¨Ø¹Ø¯ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± "Ø£Ø¬Ø¨". Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ù…Ù„ÙØ§Øª PDF ÙˆÙŠØ¹Ø±Ø¶ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„.
    3. **Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±:**
       Ø¨Ø¹Ø¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± "Ø§Ù„Ù…ØµØ§Ø¯Ø±" Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ù…Ù† Ù…Ù„ÙØ§Øª PDF Ø§Ù„ØªÙŠ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.
    4. **Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ø¦Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±:**
       ÙÙŠ Ù‚Ø³Ù… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¦Ù‡ (Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© "MCQ" Ø£Ùˆ ØµØ­/Ø®Ø·Ø£ "True/False").
       Ø­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¤Ø´Ø±ØŒ Ø«Ù… Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø§Ø¨Ø¯Ø£ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±". Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©.
    5. **Ø¥Ù†Ø´Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ:**
       Ø¥Ø°Ø§ ÙƒÙ†Øª Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© ÙˆØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© ØªØ´ØºÙŠÙ„ YouTubeØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± "Generate Video Segment URLs". Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆÙŠØ¹Ø±Ø¶Ù‡Ø§ Ù„Ùƒ.
    6. **ØªÙ†Ø¨ÙŠÙ‡Ø§Øª ÙˆØ£Ø®Ø·Ø§Ø¡:**
       Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡ØªÙƒ Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡ØŒ Ø³ØªØ¸Ù‡Ø± Ø±Ø³Ø§Ø¦Ù„ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ØªØ´Ø±Ø­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŒ Ù…Ø«Ù„ Ø¹Ø¯Ù… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª PDFØŒ Ø£Ùˆ Ø¹Ø¯Ù… Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø£Ùˆ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.
    """)

st.write("---")

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø­Ø§Øª ÙØ§Ø±ØºØ© Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ù„ØªÙˆØ³ÙŠØ· Ø§Ù„Ø²Ø± Ø¹Ù…ÙˆØ¯ÙŠÙ‹Ø§
st.write("")
st.write("")
st.write("")

# Ø²Ø± "Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
if st.button('ğŸš€ Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ğŸš€'):
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙÙŠ session_state Ù„Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
    st.session_state.vector_stores = {}
    st.session_state.reference_texts_store = {}
    st.session_state.document_store = []
    st.session_state.response_submitted = False
    st.session_state.sources_shown = False
    st.session_state.chat_history = []  # Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©

    with st.spinner('Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...'):
       process_lessons_and_video()  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    st.session_state.processing_complete = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

st.write("---")

# Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ response_form ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
if st.session_state.processing_complete:
    with st.form(key='response_form'):
        query = st.text_input("ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ:")
        response_button = st.form_submit_button(label='Ø£Ø¬Ø¨')

        if response_button:
            query_request = QueryRequest(query=query)
            response = generate_response(query_request)
            st.write("Ø§Ù„Ø±Ø¯:", response)
            st.session_state.response_submitted = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø±Ø¯

    st.write("---")

    # Ø¥Ø¸Ù‡Ø§Ø± Ø²Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙÙ‚Ø· Ø¨Ø¹Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø±Ø¯
    if st.session_state.response_submitted:
        if st.session_state.get("vector_stores") and st.button("Ø§Ù„Ù…ØµØ§Ø¯Ø±"):
            reference_texts = generate_reference_texts()
            st.write("Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨:", reference_texts)
            st.session_state.sources_shown = True  # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±

        st.write("---")

        # Ø¥Ø¸Ù‡Ø§Ø± Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø¨Ø¹Ø¯ Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
        if st.session_state.sources_shown:
            # Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            with st.form(key='questions_form'):
                question_type = st.selectbox("Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„:", ["MCQ", "True/False"])
                questions_number = st.number_input("Ø§Ø®ØªØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:", min_value=1, max_value=10)
                generate_questions_button = st.form_submit_button(label='Ø§Ø¨Ø¯Ø£ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±')

                if generate_questions_button:
                    question_request = QuestionRequest(question_type=question_type, questions_number=questions_number)
                    questions = generate_questions_endpoint(question_request)
                    st.write("Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", questions)

            st.write("---")

            # Ø²Ø± Ù„ØªÙˆÙ„ÙŠØ¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            if st.session_state.get("reference_texts_store") and st.button("Generate Video Segment URLs"):
                video_segment_urls = generate_video_segment_url()
                st.write("Generated Video Segment URLs:", video_segment_urls)
